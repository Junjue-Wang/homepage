<!DOCTYPE html>

<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>EarthVQA</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        img {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        table,
        th,
        td {
            border: 1px solid black;
            border-collapse: collapse;
        }
    </style>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- <base href="/"> -->

    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <link rel="icon" type="image/png" href="https://github.com/Junjue-Wang/FactSeg/blob/master/imgs/myicon.jpg?raw=true">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-110862391-3');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                EarthVQA: Towards Queryable <span style="color: red">Earth</span> via Relational Reasoning-Based Remote Sensing
                <span style="color: red">V</span>isual <span style="color: red">Q</span>uestion <span style="color: red">A</span>nswering</br>
                <small>
                    AAAI 2024
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://junjue-wang.github.io/homepage/">
                            Junjue Wang
                        </a>
                    </li>
                    <li>
                        <a href="http://zhuozheng.top/">
                            Zhuo Zheng
                        </a>
                    </li>
                    <li>
                          Zihang Chen
                    </li>
                    <li>
                          Ailong Ma
                    </li>
                    <li>
                        <a href="http://rsidea.whu.edu.cn/">
                          Yanfei Zhong
                        </a>
                    </li>

                </ul>
                <small>

                </small>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://www.researchgate.net/publication/376519677_EarthVQA_Towards_Queryable_Earth_via_Relational_Reasoning-based_Remote_Sensing_Visual_Question_Answering">
                            <image src="research_src/EarthVQA/paper_overview.png" height="120px"></image><br>
                                <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/Junjue-Wang/EarthVQA">
                            <image src="research_src/EarthVQA/EarthVQA-logo.png" height="120px"></image><br>
                                <h4><strong>Project</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://s3.amazonaws.com/pf-user-files-01/u-59356/uploads/2024-01-08/2q83o0t/EarthVQA-video.mp4">
                            <image src="research_src/EarthVQA/video.png" height="120px"></image><br>
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="research_src/EarthVQA/dataset_vis.png" class="img-responsive" alt="overview"><br></image>
                <image src="research_src/EarthVQA/framework.png" class="img-responsive" alt="overview"><br></image>
                <p class="text-justify">This is follow-up work of our <a href="https://Junjue-Wang.github.io/homepage/LoveDA" style="color: #0000cc">LoveDA (NeurIPS2021)</a>
                    </p>
                    <p class="text-justify">
                        Earth vision research typically focuses on extracting geospatial object locations and categories but
                        neglects the exploration of relations between objects and comprehensive reasoning.
                        Based on city planning needs, we develop a multi-modal multi-task VQA dataset (<b>EarthVQA</b>) to advance relational relational-based judging, counting, and comprehensive analysis.
                        The EarthVQA dataset contains 6000 images, corresponding semantic masks, and 208,593 QA pairs with urban and rural governance requirements embedded.
                        As objects are the basis for complex relational reasoning, we propose a Semantic OBject Awareness framework (<b>SOBA</b>) to advance VQA in an object-centric way.
                        To preserve refined spatial locations and semantics, SOBA leverages a segmentation network for object semantics generation.
                        The object-guided attention aggregates object interior features via pseudo masks, and bidirectional cross attention further models
                        object external relations hierarchically.
                        To optimize object counting, we propose a numerical difference loss that dynamically adds difference penalties, unifying the classification and regression tasks.
                        Experimental results show that SOBA outperforms both advanced general and remote sensing methods.
                        We believe this dataset and framework provide a strong benchmark for Earth vision's complex analysis.

                    </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments on EarthVQA
                </h3>
                <image src="research_src/EarthVQA/result.png" class="img-responsive" alt="overview"><br></image>

            </div>
        </div>





        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <pre data-role="codeBlock" data-info="text" class="language-text">
@article{wang2024earthvqa, 
    title={EarthVQA: Towards Queryable Earth via Relational Reasoning-Based Remote Sensing Visual Question Answering},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/28357}, 
    DOI={10.1609/ai.v38i6.28357}, 
    author={Junjue Wang and Zhuo Zheng and Zihang Chen and Ailong Ai and Yanfei Zhong}, 
    year={2024}, 
    month={Mar.},
    volume={38},
    pages={5481-5489}}
@article{earthvqanet,
    title = {EarthVQANet: Multi-task visual question answering for remote sensing image understanding},
    journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
    volume = {212},
    pages = {422-439},
    year = {2024},
    issn = {0924-2716},
    doi = {https://doi.org/10.1016/j.isprsjprs.2024.05.001},
    url = {https://www.sciencedirect.com/science/article/pii/S0924271624001990},
    author = {Junjue Wang and Ailong Ma and Zihang Chen and Zhuo Zheng and Yuting Wan and Liangpei Zhang and Yanfei Zhong}}
                </pre>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                This work was supported by National Natural Science Foundation of China under Grant Nos. 42325105, 42071350, and 42171336.
                <br><br>
                The website template was borrowed from <a href="https://bowenc0221.github.io/">Bowen Cheng</a> and <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                <p></p>
            </div>
        </div>
    </div>

    </div>
</body>

</html>
